{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"1PSBINi4Rs6ui0-wij3TklWCBYArLL10f","authorship_tag":"ABX9TyMLjeQluYQEzQvlPpYeYuPJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 1. Setup & Configuration"],"metadata":{"id":"w_vACQ8rxDqZ"}},{"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, GlobalAveragePooling1D, Dense, Dropout, Add, Bidirectional, LSTM, MaxPooling1D\n","from tensorflow.keras.regularizers import l2"],"metadata":{"id":"KsZvPbONxHJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •\n","PROJECT_ROOT = '/content/drive/MyDrive/Colab Notebooks/HAR_Research_Project'\n","sys.path.append(PROJECT_ROOT)"],"metadata":{"id":"nlPXQOsYxMK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# utils.py ë¡œë“œ\n","from utils import (\n","    squeeze_excite_block, cbam_block, transformer_encoder, categorical_focal_loss,\n","    spatial_shape, cbam_reduce_mean, cbam_reduce_max # [ì¶”ê°€]\n",")"],"metadata":{"id":"2LpaGJhB0nnp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.1. ì‹¤í—˜ë§ˆë‹¤ ë³€ê²½í•  ë³€ìˆ˜ë“¤"],"metadata":{"id":"0jpcUjK3xTVx"}},{"cell_type":"code","source":["# 1. ë°ì´í„°ì…‹: 'UCI', 'WISDM', 'UniMiB', 'PAMAP2', 'Opportunity'\n","TARGET_DATASET = 'UCI'\n","\n","# 2. ëª¨ë¸: 'standard_resnet', 'cnn_bilstm', 'se_resnet', 'cbam_resnet', 'resnet_transformer'\n","TARGET_MODEL = 'standard_resnet'\n","\n","# 3. ì‹¤í—˜ íƒœê·¸ (íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€ìš©) - ë‚ ì§œ ë˜ëŠ” ì‹¤í—˜ íŠ¹ì§• ê¸°ì…\n","EXPERIMENT_TAG = '20251204'\n","\n","# 4. í•˜ì´í¼íŒŒë¼ë¯¸í„°\n","EPOCHS = 150\n","BATCH_SIZE = 64\n","LEARNING_RATE = 0.0005\n","\n","# íŒŒì¼ ID ìƒì„± (ìë™)\n","FILE_ID = f\"{TARGET_DATASET}_{TARGET_MODEL}_{EXPERIMENT_TAG}\"\n","print(f\"ğŸš€ Experiment Initialized: {FILE_ID}\")"],"metadata":{"id":"n5D1rkJ5xaax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Data Loading & Preprocessing"],"metadata":{"id":"khuQhfqWxg1c"}},{"cell_type":"code","source":["def load_dataset_fixed(name):\n","    print(f\"\\n[Data Loading] {name} ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ (Fixed Split)...\")\n","    data_dir = f'{PROJECT_ROOT}/data/processed'\n","\n","    # 1. ê³ ì •ëœ Train/Test íŒŒì¼ ë¡œë“œ\n","    try:\n","        X_train = np.load(f'{data_dir}/{name}_X_train.npy')\n","        y_train = np.load(f'{data_dir}/{name}_y_train.npy')\n","        X_test  = np.load(f'{data_dir}/{name}_X_test.npy')\n","        y_test  = np.load(f'{data_dir}/{name}_y_test.npy')\n","    except FileNotFoundError:\n","        # íŒŒì¼ì´ ë¶„ë¦¬ ì•ˆ ë˜ì–´ ìˆëŠ” ê²½ìš°(ë‹¤ë¥¸ ë°ì´í„°ì…‹) ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©\n","        print(f\"âš ï¸ ë¶„ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ í†µí•© ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.\")\n","        return load_dataset_random(name) # ê¸°ì¡´ í•¨ìˆ˜ í˜¸ì¶œ (ì½”ë“œ ì•„ë˜ì— ìœ ì§€ í•„ìš”)\n","\n","    # 2. Validation Set ìƒì„± (Train ë°ì´í„°ì˜ 20%ë¥¼ ë–¼ì–´ì„œ ì‚¬ìš©)\n","    # Test Setì€ ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ\n","    n_classes = len(np.unique(y_train))\n","    y_train_hot = tf.keras.utils.to_categorical(y_train, n_classes)\n","    y_test_hot  = tf.keras.utils.to_categorical(y_test, n_classes)\n","\n","    # Train ë‚´ì—ì„œë§Œ Val ë¶„ë¦¬ (stratify ì ìš©)\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_train, y_train_hot, test_size=0.2, random_state=42, stratify=y_train\n","    )\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test_hot, n_classes"],"metadata":{"id":"piOIBplIiffc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_dataset_random(name):\n","    print(f\"\\n[Data Loading] {name} ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...\")\n","    data_dir = f'{PROJECT_ROOT}/data/processed'\n","\n","    try:\n","        X = np.load(f'{data_dir}/{name}_X.npy')\n","        y = np.load(f'{data_dir}/{name}_y.npy')\n","    except FileNotFoundError:\n","        raise FileNotFoundError(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {data_dir} í™•ì¸ í•„ìš”.\")\n","\n","    # [Safety] ê·¹ì†Œìˆ˜ í´ë˜ìŠ¤ í•„í„°ë§ (Stratify ì—ëŸ¬ ë°©ì§€)\n","    unique, counts = np.unique(y, return_counts=True)\n","    if np.min(counts) < 5:\n","        print(\"âš ï¸ ê·¹ì†Œìˆ˜ ìƒ˜í”Œ í´ë˜ìŠ¤ ì œê±° ì¤‘...\")\n","        mask = np.isin(y, unique[counts >= 5])\n","        X, y = X[mask], y[mask]\n","        # ë¼ë²¨ ë¦¬ë§¤í•‘\n","        label_map = {old: new for new, old in enumerate(np.unique(y))}\n","        y = np.vectorize(label_map.get)(y)\n","\n","    # One-hot Encoding\n","    n_classes = len(np.unique(y))\n","    y_cat = tf.keras.utils.to_categorical(y, n_classes)\n","\n","    # Train/Val/Test Split (6:2:2) with Error Handling\n","    try:\n","        X_train, X_temp, y_train, y_temp = train_test_split(\n","            X, y_cat, test_size=0.4, random_state=42, stratify=y\n","        )\n","        X_val, X_test, y_val, y_test = train_test_split(\n","            X_temp, y_temp, test_size=0.5, random_state=42, stratify=np.argmax(y_temp, axis=1)\n","        )\n","    except ValueError:\n","        print(\"âš ï¸ Stratify failed (Data imbalance). Switching to random split.\")\n","        X_train, X_temp, y_train, y_temp = train_test_split(X, y_cat, test_size=0.4, random_state=42)\n","        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test, n_classes"],"metadata":{"id":"SL5Tk43pxiei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if TARGET_DATASET == 'UCI':\n","    # UCIëŠ” íŒŒì¼ì´ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ Fixed í•¨ìˆ˜ í˜¸ì¶œ\n","    X_train, y_train, X_val, y_val, X_test, y_test, n_classes = load_dataset_fixed(TARGET_DATASET)\n","else:\n","    # ë‚˜ë¨¸ì§€ëŠ” ì•„ì§ íŒŒì¼ ë¶„ë¦¬ë¥¼ ì•ˆ í–ˆìœ¼ë¯€ë¡œ Random í•¨ìˆ˜ í˜¸ì¶œ\n","    X_train, y_train, X_val, y_val, X_test, y_test, n_classes = load_dataset_random(TARGET_DATASET)"],"metadata":{"id":"5LuE3S-kjKhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = StandardScaler()\n","\n","N_train, T, F = X_train.shape\n","X_train_2d = X_train.reshape(-1, F)\n","\n","# Train ë°ì´í„°ë¡œë§Œ fit (Data Leakage ë°©ì§€)\n","scaler.fit(X_train_2d)\n","\n","X_train = scaler.transform(X_train_2d).reshape(X_train.shape)\n","X_val   = scaler.transform(X_val.reshape(-1, F)).reshape(X_val.shape)\n","X_test  = scaler.transform(X_test.reshape(-1, F)).reshape(X_test.shape)\n","\n","print(f\"âœ… Data Ready: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}\")\n","print(f\"Preprocessing Info: Window Size={T}, Channels={F}, No Augmentation\")"],"metadata":{"id":"LkTWlT4Xjh0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Model Architecture Builder"],"metadata":{"id":"jfTyr5U4xnyx"}},{"cell_type":"code","source":["def resnet_block(inputs, filters, stride=1, attention=None):\n","    shortcut = inputs\n","    if stride > 1 or inputs.shape[-1] != filters:\n","        shortcut = Conv1D(filters, 1, strides=stride, padding='same')(inputs)\n","        shortcut = BatchNormalization()(shortcut)\n","\n","    x = Conv1D(filters, 3, strides=stride, padding='same', kernel_regularizer=l2(5e-5))(inputs)\n","    x = BatchNormalization()(x)\n","    x = Activation('swish')(x)\n","\n","    x = Conv1D(filters, 3, strides=1, padding='same', kernel_regularizer=l2(5e-5))(x)\n","    x = BatchNormalization()(x)\n","\n","    if attention == 'se': x = squeeze_excite_block(x)\n","    elif attention == 'cbam': x = cbam_block(x)\n","\n","    x = Add()([x, shortcut])\n","    x = Activation('swish')(x)\n","    return x"],"metadata":{"id":"tcNOi1WqxqxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_universal_model(input_shape, n_classes, model_type):\n","    inputs = Input(shape=input_shape)\n","\n","    # [Type A] CNN + Bi-LSTM\n","    if model_type == 'cnn_bilstm':\n","        x = Conv1D(64, 3, padding='same', activation='swish')(inputs)\n","        x = BatchNormalization()(x)\n","        x = MaxPooling1D(2)(x)\n","        x = Conv1D(128, 3, padding='same', activation='swish')(x)\n","        x = BatchNormalization()(x)\n","        x = MaxPooling1D(2)(x)\n","        x = Bidirectional(LSTM(64, return_sequences=False))(x)\n","        x = Dropout(0.4)(x)\n","        outputs = Dense(n_classes, activation='softmax')(x)\n","        return Model(inputs, outputs, name='CNN_BiLSTM')\n","\n","    # [Type B] ResNet Variants\n","    x = Conv1D(64, 7, strides=2, padding='same', kernel_regularizer=l2(5e-5))(inputs)\n","    x = BatchNormalization()(x)\n","    x = Activation('swish')(x)\n","\n","    att = None\n","    if 'se' in model_type: att = 'se'\n","    if 'cbam' in model_type: att = 'cbam'\n","\n","    x = resnet_block(x, 64, 1, att)\n","    x = resnet_block(x, 128, 2, att)\n","    x = resnet_block(x, 256, 2, att)\n","\n","    if 'transformer' in model_type:\n","        x = transformer_encoder(x, 64, 4, 128, 0.2)\n","        x = GlobalAveragePooling1D()(x)\n","    else:\n","        x = GlobalAveragePooling1D()(x)\n","\n","    x = Dropout(0.3)(x)\n","    outputs = Dense(n_classes, activation='softmax')(x)\n","    return Model(inputs, outputs, name=f'{model_type.upper()}')"],"metadata":{"id":"YF5JtV5txr3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ëª¨ë¸ ìƒì„±\n","model = build_universal_model(X_train.shape[1:], n_classes, TARGET_MODEL)"],"metadata":{"id":"xN4n0nu8xvfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"GaIjNSsdxwn3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Training Loop"],"metadata":{"id":"FvAZL7MBxyuj"}},{"cell_type":"code","source":["model.compile(\n","    loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipvalue=0.5),\n","    metrics=['accuracy']\n",")"],"metadata":{"id":"jKPcpnpbx0pt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = f'{PROJECT_ROOT}/checkpoints/best_model_{FILE_ID}.keras'"],"metadata":{"id":"eaVBOysix9AW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\nâš¡ Start Training: {FILE_ID} ...\")\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_val, y_val),\n","    epochs=EPOCHS,\n","    batch_size=BATCH_SIZE,\n","    verbose=2,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n","        tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy')\n","    ]\n",")"],"metadata":{"id":"62cqWUKjx99O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# í•™ìŠµ ì´ë ¥ CSV ì €ì¥\n","hist_df = pd.DataFrame(history.history)\n","hist_df.to_csv(f'{PROJECT_ROOT}/results/logs/history_{FILE_ID}.csv', index=False)"],"metadata":{"id":"jcH9yvPSx_ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Final Report & Visualization (Prof. Style + Auto Save)"],"metadata":{"id":"OhOE-HTVyBvG"}},{"cell_type":"code","source":["# 1. Best Model ë¡œë“œ (Evaluation Mode)\n","best_model = load_model(\n","    checkpoint_path,\n","    custom_objects={\n","        'focal_loss_fixed': categorical_focal_loss(),\n","        'squeeze_excite_block': squeeze_excite_block,\n","        'cbam_block': cbam_block,\n","        'transformer_encoder': transformer_encoder,\n","        'spatial_shape': spatial_shape,\n","        'cbam_reduce_mean': cbam_reduce_mean,\n","        'cbam_reduce_max': cbam_reduce_max\n","    },\n","    compile=False,\n","    safe_mode=False\n",")\n","\n","# 2. ì˜ˆì¸¡ ìˆ˜í–‰\n","y_pred_probs = best_model.predict(X_test, verbose=0)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","y_true = np.argmax(y_test, axis=1)\n","\n","# 3. ë¼ë²¨ ì •ì˜\n","if TARGET_DATASET == 'UCI':\n","    labels = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n","\n","elif TARGET_DATASET == 'WISDM':\n","    labels = ['Jogging', 'Walking', 'Upstairs', 'Downstairs', 'Sitting', 'Standing']\n","\n","elif TARGET_DATASET == 'UniMiB':\n","    labels = [\n","        'Walking', 'Running', 'GoingUpS', 'GoingDownS', 'Jumping',\n","        'SittingDown', 'StandingUpFS', 'LyingDownFS', 'StandingUpFL',\n","        'FallingForw', 'FallingBack', 'FallingRight', 'FallingLeft',\n","        'Syncope', 'FallingBackSC', 'FallingWithPS', 'HittingObstacle'\n","    ]\n","\n","elif TARGET_DATASET == 'PAMAP2':\n","    labels = [\n","        'Lying', 'Sitting', 'Standing', 'Walking', 'Running', 'Cycling',\n","        'Nordic_Walk', 'Ascending', 'Descending', 'Vacuum', 'Ironing', 'Rope_Jump'\n","    ]\n","\n","elif TARGET_DATASET == 'Opportunity':\n","    labels = ['Stand', 'Walk', 'Sit', 'Lie']\n","\n","else:\n","    # ì˜ˆì™¸ ì²˜ë¦¬: ë¼ë²¨ ê°œìˆ˜ë§Œí¼ ìˆ«ìë¡œ í‘œì‹œ\n","    labels = [str(i) for i in range(n_classes)]"],"metadata":{"id":"qrehnpH280xI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# [ìƒì„¸ ë¶„ì„ 1] ì „ì²´ ì„±ëŠ¥ ì§€í‘œ (Overall Metrics)\n","# -----------------------------------------------------------------------------\n","accuracy = accuracy_score(y_true, y_pred)\n","# Macro: í´ë˜ìŠ¤ë³„ ì ìˆ˜ì˜ ë‹¨ìˆœ í‰ê·  (ì†Œìˆ˜ í´ë˜ìŠ¤ ì„±ëŠ¥ ì¤‘ì‹œ)\n","f1_macro = f1_score(y_true, y_pred, average='macro')\n","# Weighted: ìƒ˜í”Œ ìˆ˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘” í‰ê·  (ì „ì²´ì ì¸ ì„±ëŠ¥ ì¤‘ì‹œ)\n","f1_weighted = f1_score(y_true, y_pred, average='weighted')\n","\n","print(f\"Dataset: {TARGET_DATASET} | Model: {TARGET_MODEL}\")\n","print(\"-\" * 40)\n","print(f\"âœ… Final Test Accuracy:   {accuracy:.4f}\")\n","print(f\"âœ… F1-Score (Macro):      {f1_macro:.4f}\")\n","print(f\"âœ… F1-Score (Weighted):   {f1_weighted:.4f}\")"],"metadata":{"id":"YCfwxqhH851r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# [ìƒì„¸ ë¶„ì„ 2] í´ë˜ìŠ¤ë³„ ì„¸ë¶€ ì„±ì í‘œ (Class-wise Report)\n","# -----------------------------------------------------------------------------\n","# classification_reportë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°›ì•„ DataFrameìœ¼ë¡œ ë³€í™˜\n","clf_report_dict = classification_report(y_true, y_pred, target_names=labels, output_dict=True)\n","df_report = pd.DataFrame(clf_report_dict).transpose()\n","\n","# ë¶ˆí•„ìš”í•œ í–‰(accuracy ë“±) ì •ë¦¬ ë° ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…\n","print(\"\\nğŸ“‹ [Class-wise Performance Report]\")\n","display(df_report)\n","\n","# CSVë¡œ ì €ì¥\n","df_report.to_csv(f'{PROJECT_ROOT}/results/logs/detail_report_{FILE_ID}.csv')\n","print(f\"   -> Saved details to: results/logs/detail_report_{FILE_ID}.csv\")"],"metadata":{"id":"aqu12b1m9Dix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# [ìƒì„¸ ë¶„ì„ 3] Confusion Matrix (Raw & Normalized)\n","# -----------------------------------------------------------------------------\n","plt.figure(figsize=(20, 8))\n","\n","# 3-1. Raw Counts (ê°œìˆ˜ í™•ì¸ìš©)\n","plt.subplot(1, 2, 1)\n","cm = confusion_matrix(y_true, y_pred)\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n","plt.title(f'Confusion Matrix (Counts) - {TARGET_DATASET}')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","\n","# 3-2. Normalized (ë¹„ìœ¨ í™•ì¸ìš© - í—·ê°ˆë¦¬ëŠ” ì •ë„ íŒŒì•…)\n","plt.subplot(1, 2, 2)\n","cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Reds', xticklabels=labels, yticklabels=labels)\n","plt.title(f'Confusion Matrix (Normalized) - {TARGET_DATASET}')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","\n","plt.tight_layout()\n","plt.savefig(f'{PROJECT_ROOT}/results/figures/cm_{FILE_ID}.png')\n","print(f\"   -> Saved CM plots to: results/figures/cm_{FILE_ID}.png\")\n","plt.show()"],"metadata":{"id":"0M-e2wT29FLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----------------------------------------------------------------------------\n","# [ìƒì„¸ ë¶„ì„ 4] í•™ìŠµ ê³¡ì„  (Loss & Accuracy)\n","# -----------------------------------------------------------------------------\n","plt.figure(figsize=(14, 5))\n","\n","# Accuracy\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Train Acc', linewidth=2)\n","plt.plot(history.history['val_accuracy'], label='Val Acc', linewidth=2)\n","plt.title(f'Model Accuracy - {TARGET_MODEL}')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')\n","plt.grid(True, alpha=0.3)\n","\n","# Loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n","plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n","plt.title(f'Model Loss - {TARGET_MODEL}')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(loc='upper right')\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f'{PROJECT_ROOT}/results/figures/learning_curve_{FILE_ID}.png')\n","plt.show()"],"metadata":{"id":"5O3Q8nAF9HRm"},"execution_count":null,"outputs":[]}]}