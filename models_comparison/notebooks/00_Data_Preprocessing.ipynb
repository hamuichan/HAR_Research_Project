{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1gpJeGbGUgvmcltVgMIGWDd-9FDxxLIPp","authorship_tag":"ABX9TyO3ZiKiB49tgkoak7O2AcFM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","from scipy import stats\n","from glob import glob\n","from tqdm import tqdm"],"metadata":{"id":"8m2n3NBxvkLs","executionInfo":{"status":"ok","timestamp":1764787470740,"user_tz":-540,"elapsed":2166,"user":{"displayName":"í•¨ì˜ì°¬","userId":"05008808075825073096"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_Research_Project/data/raw'  # ì›ë³¸ ë°ì´í„°ê°€ ìˆëŠ” ìµœìƒìœ„ í´ë”\n","SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_Research_Project/data/processed'  # ì „ì²˜ë¦¬ëœ .npy íŒŒì¼ì„ ì €ì¥í•  í´ë”\n","\n","# ì €ì¥ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n","if not os.path.exists(SAVE_PATH):\n","    os.makedirs(SAVE_PATH)\n","\n","# ê³µí†µ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n","TIME_STEPS = 128\n","STEP = 64  # Overlap 50% (128ì˜ ì ˆë°˜)"],"metadata":{"id":"wSZ8gv2UvnKB","executionInfo":{"status":"ok","timestamp":1764787473039,"user_tz":-540,"elapsed":1001,"user":{"displayName":"í•¨ì˜ì°¬","userId":"05008808075825073096"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# [ìœ í‹¸ë¦¬í‹°] ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± í•¨ìˆ˜\n","# ì—°ì†ëœ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê¸° ì¢‹ê²Œ (Samples, 128, Channels)ë¡œ ìë¦„\n","# ==========================================\n","def create_segments(data, labels, time_steps, step):\n","    segments = []\n","    labels_list = []\n","\n","    # ë°ì´í„° ê¸¸ì´ê°€ ìœˆë„ìš°ë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ìŠ¤\n","    if len(data) < time_steps:\n","        return np.empty((0, time_steps, data.shape[1])), np.empty((0,))\n","\n","    # Stepë§Œí¼ ì´ë™í•˜ë©° ìœˆë„ìš° ìë¥´ê¸°\n","    for i in range(0, len(data) - time_steps, step):\n","        xs = data[i : i + time_steps]\n","        # í•´ë‹¹ ìœˆë„ìš°ì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë¼ë²¨(Mode)ì„ ì •ë‹µìœ¼ë¡œ ì±„íƒ\n","        ys = stats.mode(labels[i : i + time_steps], keepdims=True)[0][0]\n","        segments.append(xs)\n","        labels_list.append(ys)\n","\n","    return np.array(segments), np.array(labels_list)"],"metadata":{"id":"ySWg-j2QvsC0","executionInfo":{"status":"ok","timestamp":1764787476021,"user_tz":-540,"elapsed":4,"user":{"displayName":"í•¨ì˜ì°¬","userId":"05008808075825073096"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 1. UCI-HAR ì „ì²˜ë¦¬\n","# íŠ¹ì§•: ì´ë¯¸ 128ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë˜ì–´ ìˆìŒ. txt íŒŒì¼ì„ ì½ì–´ì„œ í•©ì¹˜ê¸°ë§Œ í•˜ë©´ ë¨.\n","# ==========================================\n","def process_uci_har():\n","    print(\"\\n[1/5] Processing UCI-HAR...\")\n","    dataset_path = os.path.join(BASE_PATH, 'UCI_HAR_Dataset')\n","\n","    if not os.path.exists(dataset_path):\n","        print(\"âŒ Skipping UCI-HAR (Folder not found)\")\n","        return\n","\n","    # ì½ì–´ì˜¬ ì‹ í˜¸ íŒŒì¼ ëª©ë¡ (ì´ 9ì±„ë„: Body Acc, Body Gyro, Total Acc)\n","    # ìˆœì„œ: train/test í´ë” ì•ˆì˜ Inertial Signals í´ë”ì— ìˆìŒ\n","    signal_files = [\n","        'body_acc_x_', 'body_acc_y_', 'body_acc_z_',\n","        'body_gyro_x_', 'body_gyro_y_', 'body_gyro_z_',\n","        'total_acc_x_', 'total_acc_y_', 'total_acc_z_'\n","    ]\n","\n","    def load_group(group):\n","        # group: 'train' or 'test'\n","        signals_data = []\n","        path = os.path.join(dataset_path, group, 'Inertial Signals')\n","\n","        for file_name in signal_files:\n","            file_path = os.path.join(path, f\"{file_name}{group}.txt\")\n","            # ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°\n","            signals_data.append(\n","                pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","            )\n","\n","        # (Channels, Samples, 128) -> (Samples, 128, Channels)ë¡œ ë³€í™˜\n","        # np.dstackì€ ê¹Šì´(3ë²ˆì§¸ ì°¨ì›) ë°©í–¥ìœ¼ë¡œ ìŒ“ìŒ\n","        return np.dstack(signals_data)\n","\n","    def load_y(group):\n","        file_path = os.path.join(dataset_path, group, f\"y_{group}.txt\")\n","        return pd.read_csv(file_path, delim_whitespace=True, header=None).values.flatten() - 1 # 0-based index\n","\n","    try:\n","        # í•©ì¹˜ì§€ ì•Šê³  ê°ê° ë¡œë“œ ë° ì €ì¥\n","        X_train = load_group('train')\n","        y_train = load_y('train')\n","        X_test = load_group('test')\n","        y_test = load_y('test')\n","\n","        # Train / Test ë³„ë„ ì €ì¥\n","        np.save(f'{SAVE_PATH}/UCI_X_train.npy', X_train)\n","        np.save(f'{SAVE_PATH}/UCI_y_train.npy', y_train)\n","        np.save(f'{SAVE_PATH}/UCI_X_test.npy', X_test)\n","        np.save(f'{SAVE_PATH}/UCI_y_test.npy', y_test)\n","\n","        print(f\"âœ… UCI-HAR Done (Fixed Split).\")\n","        print(f\"   Train: {X_train.shape}, Test: {X_test.shape}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error UCI-HAR: {e}\")"],"metadata":{"id":"9x22An_cv7wn","executionInfo":{"status":"ok","timestamp":1764787477542,"user_tz":-540,"elapsed":7,"user":{"displayName":"í•¨ì˜ì°¬","userId":"05008808075825073096"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 2. WISDM ì „ì²˜ë¦¬\n","# íŠ¹ì§•: í…ìŠ¤íŠ¸ íŒŒì¼ ëì— ì„¸ë¯¸ì½œë¡ (;)ì´ ìˆì–´ íŒŒì‹± ì—ëŸ¬ ë°œìƒ -> ì œê±° í•„ìš”\n","# ==========================================\n","def process_wisdm():\n","    print(\"\\n[2/5] Processing WISDM...\")\n","    file_path = os.path.join(BASE_PATH, 'WISDM_ar_v1.1', 'WISDM_ar_v1.1_raw.txt')\n","\n","    if not os.path.exists(file_path):\n","        print(f\"âŒ Skipping WISDM (File not found)\")\n","        return\n","\n","    columns = ['user', 'activity', 'timestamp', 'x', 'y', 'z']\n","\n","    try:\n","        # on_bad_lines='skip': í˜•ì‹ì´ ì˜ëª»ëœ ë¼ì¸ì€ ê±´ë„ˆëœ€\n","        df = pd.read_csv(file_path, header=None, names=columns, on_bad_lines='skip')\n","\n","        # [í•µì‹¬] Zì¶• ë°ì´í„° ë’¤ì— ë¶™ì€ ';' ì œê±°í•˜ê³  ì‹¤ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜\n","        df['z'] = df['z'].astype(str).str.replace(';', '', regex=False).astype(float)\n","        df.dropna(inplace=True)\n","\n","        # ë¬¸ìì—´ ë¼ë²¨(Jogging ë“±)ì„ ìˆ«ìë¡œ ë³€í™˜\n","        activities = df['activity'].unique()\n","        label_map = {label: i for i, label in enumerate(activities)}\n","        df['activity_id'] = df['activity'].map(label_map)\n","\n","        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©\n","        X, y = create_segments(df[['x', 'y', 'z']].values, df['activity_id'].values, TIME_STEPS, STEP)\n","\n","        np.save(f'{SAVE_PATH}/WISDM_X.npy', X)\n","        np.save(f'{SAVE_PATH}/WISDM_y.npy', y)\n","        print(f\"âœ… WISDM Done. Shape: {X.shape}, Classes: {len(activities)}\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Error WISDM: {e}\")"],"metadata":{"id":"RZ6kJYIGwDrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 3. UniMiB SHAR ì „ì²˜ë¦¬\n","# íŠ¹ì§•: Matlab ë°ì´í„°ê°€ í‰íƒ„í™”ë˜ì–´ ìˆê±°ë‚˜ ì¶•ì´ ê¼¬ì—¬ ìˆìŒ. (N, 3, 151) -> (N, 128, 3)\n","# ==========================================\n","def process_unimib():\n","    print(\"\\n[3/5] Processing UniMiB SHAR (Direct Access Mode)...\")\n","    file_path = os.path.join(BASE_PATH, 'UniMiB_SHAR_Dataset', 'full_data.npy')\n","\n","    if not os.path.exists(file_path):\n","        print(f\"âŒ Skipping UniMiB (full_data.npy not found)\")\n","        return\n","\n","    try:\n","        # 1. ë°ì´í„° ë¡œë“œ\n","        full_data = np.load(file_path, allow_pickle=True)\n","        # (30, 5) -> 150 items (ê° í”¼í—˜ìë³„ ì‹¤í—˜ ë°ì´í„°)\n","        flat_data = full_data.flatten()\n","\n","        class_names = [\n","            'Walking', 'Running', 'GoingUpS', 'GoingDownS', 'Jumping',\n","            'SittingDown', 'StandingUpFS', 'LyingDownFS', 'StandingUpFL',\n","            'FallingForw', 'FallingBack', 'FallingRight', 'FallingLeft',\n","            'Syncope', 'FallingBackSC', 'FallingWithPS', 'HittingObstacle'\n","        ]\n","\n","        X_list = []\n","        y_list = []\n","\n","        # 2. ë°ì´í„° ìˆœíšŒ (ì´ 150ê°œ)\n","        for item_index, item_wrapper in enumerate(flat_data):\n","            # [Step 1] (1, 1) ê»ì§ˆ ê°•ì œ í•´ì œ\n","            # ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ (1, 1) ì•ˆì— ìˆ¨ì–´ ìˆìŒ\n","            try:\n","                if isinstance(item_wrapper, np.ndarray) and item_wrapper.shape == (1, 1):\n","                    item = item_wrapper[0, 0]\n","                else:\n","                    item = item_wrapper\n","            except:\n","                continue\n","\n","            # [Step 2] êµ¬ì¡°ì²´ í•„ë“œ í™•ì¸\n","            if not (hasattr(item, 'dtype') and item.dtype.names):\n","                continue\n","\n","            # [Step 3] ê° í´ë˜ìŠ¤ë³„ ë°ì´í„° ì¶”ì¶œ\n","            for label_id, name in enumerate(class_names):\n","                if name in item.dtype.names:\n","                    # 'Walking' ë°ì´í„° êº¼ë‚´ê¸° (ì˜ˆìƒ: (2, 1) ë°°ì—´)\n","                    container = item[name]\n","\n","                    # (2, 1) í˜•íƒœì˜ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ íƒìƒ‰\n","                    if isinstance(container, np.ndarray):\n","                        # flatten()ìœ¼ë¡œ (2, 1) -> [ìš”ì†Œ1, ìš”ì†Œ2] ë¡œ ë§Œë“¤ì–´ì„œ ì „ìˆ˜ ì¡°ì‚¬\n","                        sub_items = container.flatten()\n","\n","                        for real_data in sub_items:\n","                            # [í•µì‹¬] ë°ì´í„°ê°€ ìœ íš¨í•œì§€ í™•ì¸ (Shape (6, N))\n","                            if isinstance(real_data, np.ndarray) and real_data.ndim == 2:\n","                                if real_data.shape[0] == 6:\n","                                    # ë°ì´í„° ë°œê²¬! (ê°€ì†ë„ X, Y, ZëŠ” 0, 1, 2í–‰)\n","                                    # (6, Time) -> (3, Time) -> (Time, 3)\n","                                    raw_signal = real_data[:3, :].T\n","\n","                                    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©\n","                                    dummy_labels = np.full(len(raw_signal), label_id)\n","                                    xs, ys = create_segments(raw_signal, dummy_labels, TIME_STEPS, STEP)\n","\n","                                    if len(xs) > 0:\n","                                        X_list.append(xs)\n","                                        y_list.append(ys)\n","\n","        # 4. ë³‘í•© ë° ì €ì¥\n","        if X_list:\n","            X = np.concatenate(X_list)\n","            y = np.concatenate(y_list)\n","\n","            np.save(f'{SAVE_PATH}/UniMiB_X.npy', X)\n","            np.save(f'{SAVE_PATH}/UniMiB_y.npy', y)\n","            print(f\"âœ… UniMiB Done. Shape: {X.shape}, Classes: {len(np.unique(y))} (Target: 17)\")\n","        else:\n","            print(\"âŒ Error: No segments created. (Data list is empty)\")\n","            # ë””ë²„ê¹…ì„ ìœ„í•´ ì²« ë²ˆì§¸ ì•„ì´í…œì˜ ìƒíƒœ ì¶œë ¥\n","            print(\"DEBUG: First item structure check failed.\")\n","\n","    except Exception as e:\n","        print(f\"âŒ Critical Error UniMiB: {e}\")"],"metadata":{"id":"FvREAAvvwFP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 4. PAMAP2 ì „ì²˜ë¦¬\n","# íŠ¹ì§•: ê²°ì¸¡ì¹˜(NaN)ê°€ ë§ìŒ -> ë³´ê°„ë²• ì‚¬ìš©. ìƒ˜í”Œë§ ë ˆì´íŠ¸ê°€ ë†’ìŒ(100Hz) -> ë‹¤ìš´ìƒ˜í”Œë§.\n","# ==========================================\n","def process_pamap2():\n","    print(\"\\n[4/5] Processing PAMAP2...\")\n","    proto_path = os.path.join(BASE_PATH, 'PAMAP2_Dataset', 'Protocol')\n","\n","    if not os.path.exists(proto_path):\n","        print(\"âŒ Skipping PAMAP2 (Folder not found)\")\n","        return\n","\n","    # ì‚¬ìš©í•  ì»¬ëŸ¼ ì¸ë±ìŠ¤ (ì´ 9ì±„ë„ ê°€ì†ë„ + 1ê°œ ë¼ë²¨)\n","    # Col 1: ActivityID\n","    # Col 4-6: Hand Acc, 21-23: Chest Acc, 38-40: Ankle Acc\n","    relevant_cols = [1, 4, 5, 6, 21, 22, 23, 38, 39, 40]\n","\n","    data_collection = []\n","    label_collection = []\n","\n","    # subject101.dat ~ subject109.dat íŒŒì¼ ì°¾ê¸°\n","    files = glob(os.path.join(proto_path, '*.dat'))\n","\n","    for f in tqdm(files, desc=\"Reading PAMAP2 Files\"):\n","        try:\n","            df = pd.read_csv(f, sep=' ', header=None)\n","            df = df.iloc[:, relevant_cols]\n","\n","            # [í•µì‹¬] ê²°ì¸¡ì¹˜ ì„ í˜• ë³´ê°„ (Interpolation)\n","            df = df.interpolate(method='linear', limit_direction='both')\n","            df.dropna(inplace=True)\n","\n","            # [í•µì‹¬] ë‹¤ìš´ìƒ˜í”Œë§ (100Hz -> ~33Hz)\n","            # 3ê°œ ì¤‘ 1ê°œë§Œ ì„ íƒí•˜ì—¬ ë°ì´í„° í¬ê¸° ì¤„ì„\n","            df = df.iloc[::3, :]\n","\n","            # Activity ID 0 (Transient/Null) ì œê±°\n","            df = df[df[1] != 0]\n","\n","            # ë°ì´í„° ë¶„ë¦¬\n","            data = df.iloc[:, 1:].values # 9 Channels\n","            labels = df.iloc[:, 0].values\n","\n","            # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±\n","            X_sub, y_sub = create_segments(data, labels, TIME_STEPS, STEP)\n","\n","            if len(X_sub) > 0:\n","                data_collection.append(X_sub)\n","                label_collection.append(y_sub)\n","\n","        except Exception as e:\n","            print(f\"Error reading {os.path.basename(f)}: {e}\")\n","\n","    if data_collection:\n","        X = np.concatenate(data_collection)\n","        y = np.concatenate(label_collection)\n","\n","        # ë¼ë²¨ ë¦¬ë§¤í•‘ (ë¶ˆì—°ì†ì ì¸ IDë“¤ì„ 0ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ë§¤í•‘)\n","        unique_labels = np.unique(y)\n","        label_map = {original: new for new, original in enumerate(unique_labels)}\n","        y = np.vectorize(label_map.get)(y)\n","\n","        np.save(f'{SAVE_PATH}/PAMAP2_X.npy', X)\n","        np.save(f'{SAVE_PATH}/PAMAP2_y.npy', y)\n","        print(f\"âœ… PAMAP2 Done. Shape: {X.shape}, Classes: {len(unique_labels)}\")\n","    else:\n","        print(\"âŒ PAMAP2: No data processed.\")"],"metadata":{"id":"dVPU9mr1wGvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# 5. Opportunity ì „ì²˜ë¦¬\n","# íŠ¹ì§•: ì„¼ì„œê°€ ë§¤ìš° ë§ê³  ë³µì¡í•¨. í•„ìš”í•œ ì„¼ì„œë§Œ ì„ ë³„í•˜ê³  Null Class ì œê±° í•„ìš”.\n","# ==========================================\n","def process_opportunity():\n","    print(\"\\n[5/5] Processing Opportunity...\")\n","    data_dir = os.path.join(BASE_PATH, 'OpportunityUCIDataset')\n","\n","    # 'ADL' (ì¼ìƒ í–‰ë™) íŒŒì¼ë§Œ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰\n","    files = glob(os.path.join(data_dir, '**', '*.dat'), recursive=True)\n","    files = [f for f in files if 'ADL' in os.path.basename(f)]\n","\n","    if not files:\n","        print(\"âŒ Skipping Opportunity (No ADL files found)\")\n","        return\n","\n","    # ì‚¬ìš©í•  ì»¬ëŸ¼ ì¸ë±ìŠ¤ (Opportunity ë¬¸ì„œ ë° ë°ì´í„° ì •ì°° ê²°ê³¼ ê¸°ë°˜)\n","    # Back Acc: 37,38,39 / RUA Acc: 50,51,52 / Label(Locomotion): 243\n","    feature_cols = [37, 38, 39, 50, 51, 52]\n","    label_col = 243\n","    use_cols = feature_cols + [label_col]\n","\n","    data_collection = []\n","    label_collection = []\n","\n","    for f in tqdm(files, desc=\"Reading Opportunity Files\"):\n","        try:\n","            # ê³µë°± êµ¬ë¶„, í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½\n","            df = pd.read_csv(f, header=None, delim_whitespace=True, usecols=use_cols)\n","\n","            # ê²°ì¸¡ì¹˜ ë³´ê°„\n","            df = df.interpolate(method='linear', limit_direction='both')\n","            df.dropna(inplace=True)\n","\n","            # ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥ (Features..., Label)\n","            df = df[feature_cols + [label_col]]\n","\n","            # Null Class(0) ì œê±° (ì˜ë¯¸ ì—†ëŠ” êµ¬ê°„)\n","            df = df[df[label_col] != 0]\n","\n","            data = df.iloc[:, :-1].values\n","            labels = df.iloc[:, -1].values\n","\n","            X_sub, y_sub = create_segments(data, labels, TIME_STEPS, STEP)\n","\n","            if len(X_sub) > 0:\n","                data_collection.append(X_sub)\n","                label_collection.append(y_sub)\n","\n","        except Exception as e:\n","            print(f\"Error processing {os.path.basename(f)}: {e}\")\n","\n","    if data_collection:\n","        X = np.concatenate(data_collection)\n","        y = np.concatenate(label_collection)\n","\n","        # ë¼ë²¨ ë¦¬ë§¤í•‘ (1:Stand, 2:Walk, 4:Sit, 5:Lie -> 0,1,2,3)\n","        unique_labels = sorted(np.unique(y))\n","        label_map = {original: new for new, original in enumerate(unique_labels)}\n","        y = np.vectorize(label_map.get)(y)\n","\n","        np.save(f'{SAVE_PATH}/Opportunity_X.npy', X)\n","        np.save(f'{SAVE_PATH}/Opportunity_y.npy', y)\n","        print(f\"âœ… Opportunity Done. Shape: {X.shape}, Classes: {len(unique_labels)}\")\n","    else:\n","        print(\"âŒ Opportunity: No data processed.\")"],"metadata":{"id":"cXMKzxnPwIXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==========================================\n","# ë©”ì¸ ì‹¤í–‰ë¶€\n","# ==========================================\n","if __name__ == \"__main__\":\n","    print(f\"ğŸš€ Starting Data Preprocessing...\")\n","    print(f\"ğŸ“‚ Base Path: {BASE_PATH}\")\n","    print(f\"ğŸ’¾ Save Path: {SAVE_PATH}\")\n","\n","    process_uci_har()\n","    process_wisdm()\n","    process_unimib()\n","    process_pamap2()\n","    process_opportunity()\n","\n","    print(\"\\nğŸ‰ All Preprocessing Steps Finished!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kn0kI_QuwKHo","executionInfo":{"status":"ok","timestamp":1764787500202,"user_tz":-540,"elapsed":19308,"user":{"displayName":"í•¨ì˜ì°¬","userId":"05008808075825073096"}},"outputId":"dd2096b8-aa01-4eac-b038-18443e73f2ec"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Starting Data Preprocessing...\n","ğŸ“‚ Base Path: /content/drive/MyDrive/Colab Notebooks/HAR_Research_Project/data/raw\n","ğŸ’¾ Save Path: /content/drive/MyDrive/Colab Notebooks/HAR_Research_Project/data/processed\n","\n","[1/5] Processing UCI-HAR...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:39: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  return pd.read_csv(file_path, delim_whitespace=True, header=None).values.flatten() - 1 # 0-based index\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:30: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  pd.read_csv(file_path, delim_whitespace=True, header=None).values\n","/tmp/ipython-input-118896812.py:39: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  return pd.read_csv(file_path, delim_whitespace=True, header=None).values.flatten() - 1 # 0-based index\n"]},{"output_type":"stream","name":"stdout","text":["âœ… UCI-HAR Done (Fixed Split).\n","   Train: (7352, 128, 9), Test: (2947, 128, 9)\n","\n","ğŸ‰ All Preprocessing Steps Finished!\n"]}]}]}